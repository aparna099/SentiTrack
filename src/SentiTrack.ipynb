{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis of App Reviews for Requirement Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Paths and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for the input and output data\n",
    "\n",
    "data_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/reviews.csv'\n",
    "senti_score_reviews_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/senti_score_reviews.csv'\n",
    "nlp_processed_reviews_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/nlp_processed_reviews.csv'\n",
    "Xtrain_bow_textrep_reviews_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/Xtrain_bow_textrep_reviews1.csv'\n",
    "Xtest_bow_textrep_reviews_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/Xtest_bow_textrep_reviews1.csv'\n",
    "bow_vocabulary_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/word_indices.pkl'\n",
    "glove_vocabulary_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/glove_vocabulary.pkl'\n",
    "senti_jar_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/SentiStrength.jar'\n",
    "senti_lang_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/SentiStrength_Data/'\n",
    "vocabulary_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data//bow_vocabulary.pkl'\n",
    "Xtrain_data_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/Xtrain_reviews_path.csv'\n",
    "Xtest_data_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/Xtest_reviews_path.csv'\n",
    "pretrained_glove_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/glove.small.txt'\n",
    "Xtrain_glove_textrep_reviews_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/Xtrain_glove_textrep_reviews1.csv'\n",
    "Xtest_glove_textrep_reviews_path = 'C:/Users/aparn/Desktop/My Folder/MDS - UoM/Semester 2/Dissertation/SentimentTrack/data/Xtest_glove_textrep_reviews1.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd                    # For data manipulation and analysis\n",
    "import subprocess                      # For running external processes\n",
    "import re                              # For regular expressions\n",
    "import string                          # For string manipulation\n",
    "import nltk                            # Natural Language Toolkit for text processing\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer  # Text preprocessing tools\n",
    "from nltk.tokenize import word_tokenize  # Tokenization of text\n",
    "import numpy as np                     # For numerical computations\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # Metrics for evaluation\n",
    "from tensorflow import keras          # Deep learning framework\n",
    "from tensorflow.keras.models import Sequential  # Sequential neural network model\n",
    "from tensorflow.keras.layers import Dense  # Dense layers for neural networks\n",
    "import ast                             # For handling abstract syntax trees\n",
    "import time                            # For measuring time\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression classifier\n",
    "from sklearn.model_selection import cross_val_score  # Cross-validation scoring\n",
    "from sklearn.naive_bayes import MultinomialNB  # Multinomial Naive Bayes classifier\n",
    "import matplotlib.pyplot as plt        # For plotting\n",
    "import torch                           # PyTorch for deep learning\n",
    "import pickle                          # For serializing Python objects\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree classifier\n",
    "from keras.layers import Embedding, Flatten, Conv1D  # Keras layers for deep learning\n",
    "from sklearn import svm                # Support Vector Machine classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1: Initial Labelling of Unlabelled data using SentiStrength Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the app review data\n",
    "reviews_df = pd.read_csv(data_path, encoding='latin-1')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by converting it to lowercase, removing punctuation, and emoticons.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove emoticons (you can customize this regex pattern based on your data)\n",
    "    emoticon_pattern = re.compile(\"[\"\n",
    "                                  u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                  u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                  u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                  u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                  \"]+\", flags=re.UNICODE)\n",
    "    text = emoticon_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Define a function to apply SentiStrength sentiment analysis on each review\n",
    "def get_sentiment(review):\n",
    "    \"\"\"\n",
    "    Applies SentiStrength sentiment analysis to the given review text and returns positive and negative scores.\n",
    "\n",
    "    Args:\n",
    "        review (str): The review text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing positive and negative sentiment scores.\n",
    "    \"\"\"\n",
    "    # review = preprocess_text(review)  # Optional text preprocessing\n",
    "    process = subprocess.Popen(['java', '-jar', senti_jar_path, 'sentidata', senti_lang_path, 'text', review],\n",
    "                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    scores_str = stdout.decode('utf-8').strip()\n",
    "    if scores_str:\n",
    "        scores = scores_str.split('\\t')\n",
    "        sentiment_scores = []\n",
    "        for s in scores:\n",
    "            try:\n",
    "                pos_score, neg_score = map(int, s.split())\n",
    "                sentiment_scores.append((pos_score, neg_score))\n",
    "            except ValueError:\n",
    "                pass  # Ignore non-integer sentiment scores\n",
    "        if sentiment_scores:\n",
    "            negative_score = min([s[0] for s in sentiment_scores])\n",
    "            positive_score = max([s[1] for s in sentiment_scores])\n",
    "            return positive_score, negative_score\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "# Apply SentiStrength sentiment analysis on each review and store the scores in two new columns\n",
    "reviews_df[['negative_score', 'positive_score']] = reviews_df['review'].apply(get_sentiment).apply(pd.Series)\n",
    "\n",
    "# Display the results\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "reviews_df[['review','positive_score','negative_score']]\n",
    "\n",
    "# Save the DataFrame with sentiment scores to a CSV file\n",
    "reviews_df.to_csv(senti_score_reviews_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the app review data\n",
    "reviews_df = pd.read_csv(senti_score_reviews_path, encoding='latin-1')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by converting it to lowercase, removing punctuation, stopwords,\n",
    "    lemmatization, and stemming.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase\n",
    "    clean_text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    clean_text = remove_punctuation(clean_text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    clean_text = remove_stopwords(clean_text)\n",
    "\n",
    "    # Lemmatize the text\n",
    "    preprocessed_text = lemmatize_text(clean_text)\n",
    "\n",
    "    # Stem the text\n",
    "    preprocessed_text = stem_text(preprocessed_text)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text without punctuation.\n",
    "    \"\"\"\n",
    "    # Create a translation table with punctuation characters mapped to None\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # Remove punctuation using the translation table\n",
    "    text_without_punct = text.translate(translator)\n",
    "    \n",
    "    return text_without_punct\n",
    "\n",
    "\n",
    "def remove_stopwords(clean_text):\n",
    "    \"\"\"\n",
    "    Removes common stopwords from the given text.\n",
    "\n",
    "    Args:\n",
    "        clean_text (str): The preprocessed text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with stopwords removed.\n",
    "    \"\"\"\n",
    "    stopwords = [...]  # List of common stopwords\n",
    "    \n",
    "    tokenized_words  = clean_text.split()\n",
    "\n",
    "    # Remove the stop words from the text\n",
    "    filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
    "    \n",
    "    # Join the filtered words back into a single string\n",
    "    cleaned_text = \" \".join(filtered_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatizes the words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The lemmatized text.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    lemmatized_text = \" \".join(lemmatized_words)\n",
    "    return lemmatized_text\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    Stems the words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The stemmed text.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    stemmed_text = \" \".join(stemmed_words)\n",
    "    return stemmed_text\n",
    "\n",
    "# Apply the preprocessing functions to each review and store the results in a new column\n",
    "reviews_df[['preprocessed_text']] = reviews_df['review'].apply(preprocess_text).apply(pd.Series)\n",
    "\n",
    "# Ensure the 'preprocessed_text' column is of string data type\n",
    "reviews_df['preprocessed_text'] = reviews_df['preprocessed_text'].astype(str)\n",
    "\n",
    "# Save the DataFrame with preprocessed text to a CSV file\n",
    "reviews_df.to_csv(nlp_processed_reviews_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 3: Data Split and EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split:\n",
    "\n",
    "# Load the app review data\n",
    "reviews_df = pd.read_csv(nlp_processed_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Separate features (reviews) and labels (positive_score and negative_score)\n",
    "X = reviews_df['preprocessed_text']\n",
    "y_positive = reviews_df['positive_score'].tolist()\n",
    "y_negative = reviews_df['negative_score'].tolist()\n",
    "\n",
    "# Combine positive and negative values into a single target label\n",
    "y = [1 if abs(pos_score) > abs(neg_score) else 0 for pos_score, neg_score in zip(y_positive, y_negative)]\n",
    "reviews_df['label'] = y\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the training and test data to CSV files\n",
    "X_train.to_csv(Xtrain_data_path, index=False)\n",
    "X_test.to_csv(Xtest_data_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_train and y_test contain the labels (0 for negative and 1 for positive)\n",
    "positive_train = sum(y_train)\n",
    "negative_train = len(y_train) - positive_train\n",
    "\n",
    "positive_test = sum(y_test)\n",
    "negative_test = len(y_test) - positive_test\n",
    "\n",
    "# Data for pie charts\n",
    "train_data = [positive_train, negative_train]\n",
    "test_data = [positive_test, negative_test]\n",
    "\n",
    "# Labels for pie charts\n",
    "labels = ['Positive', 'Negative']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot for training data\n",
    "axes[0].pie(train_data, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Distribution of Sentiments in Training Data')\n",
    "\n",
    "# Plot for testing data\n",
    "axes[1].pie(test_data, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Distribution of Sentiments in Testing Data')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 4: Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW Random Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the app review data\n",
    "train_reviews_df = pd.read_csv(Xtrain_data_path, encoding='latin-1')\n",
    "\n",
    "# Define a list of punctuation marks to be removed from text\n",
    "PUNCTUATIONS = [',', '.', ';', ':', '!', '?']\n",
    "\n",
    "def BoW(training_reviews):\n",
    "    # Initialize a custom vocabulary list and count dictionary\n",
    "    custom_vocab_list = []\n",
    "    custom_vocab_count = {}\n",
    "    \n",
    "    # Initialize word_indices with default values for padding and unknown words\n",
    "    word_indices = {\"#PAD#\": 0, \"#UNK#\": 1}\n",
    "    \n",
    "    # Define the maximum sentence length\n",
    "    max_sentence_length = 600\n",
    "    \n",
    "    # Iterate through training reviews\n",
    "    for review in training_reviews:\n",
    "        if not isinstance(review, str):\n",
    "            custom_vocab_list.append(' ')  # Skip non-string elements\n",
    "        else:\n",
    "            custom_vocab_list += review.split(' ')\n",
    "\n",
    "    # Count the occurrences of each word in the custom vocabulary\n",
    "    for word in custom_vocab_list:\n",
    "        custom_vocab_count[word] = custom_vocab_count.get(word, 0) + 1\n",
    "\n",
    "    # Assign unique indices to words based on their frequency\n",
    "    idx_counter = 2\n",
    "    for word in custom_vocab_count.keys():\n",
    "        word_indices[word] = idx_counter\n",
    "        idx_counter += 1\n",
    "\n",
    "    # Create Bag of Words (BoW) vectors for each review\n",
    "    bow_vectors = [] \n",
    "    for review in training_reviews:\n",
    "        if not isinstance(review, str):\n",
    "            tokens = ['#UNK#']  # Skip non-string elements\n",
    "        else:\n",
    "            tokens = review.split()\n",
    "        \n",
    "        # Map words to their corresponding indices in the vocabulary\n",
    "        sent_indices = [word_indices.get(token, word_indices[\"#UNK#\"]) for token in tokens]\n",
    "        \n",
    "        # Pad the sequences to the specified maximum sentence length\n",
    "        if len(sent_indices) < max_sentence_length:\n",
    "            sent_indices += [word_indices[\"#PAD#\"]] * (max_sentence_length - len(sent_indices))\n",
    "        \n",
    "        # Append the BoW vector for the review\n",
    "        bow_vectors.append(sent_indices)\n",
    "    \n",
    "    return bow_vectors, word_indices\n",
    "\n",
    "# Extract preprocessed text reviews\n",
    "reviews = train_reviews_df['preprocessed_text']\n",
    "\n",
    "# Generate BoW vectors and word indices\n",
    "bow_vectors, word_indices = BoW(reviews)\n",
    "\n",
    "# Add BoW vectors to the DataFrame\n",
    "train_reviews_df['bow_vector'] = bow_vectors\n",
    "\n",
    "# Save the word_indices to a file using pickle\n",
    "with open(bow_vocabulary_path, \"wb\") as f:\n",
    "    pickle.dump(word_indices, f)\n",
    "\n",
    "# Save the DataFrame with BoW vectors\n",
    "train_reviews_df.to_csv(Xtrain_bow_textrep_reviews_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already loaded the test reviews into 'test_reviews_df'\n",
    "# Load the app review data\n",
    "test_reviews_df = pd.read_csv(Xtest_data_path, encoding='latin-1')\n",
    "\n",
    "# Load the word_indices from the file using pickle\n",
    "with open(bow_vocabulary_path, \"rb\") as f:\n",
    "    word_indices = pickle.load(f)\n",
    "\n",
    "def BoW_test(test_reviews, word_indices):\n",
    "    \"\"\"\n",
    "    Generate Bag of Words (BoW) representations for a list of preprocessed test reviews using pre-loaded word indices.\n",
    "\n",
    "    Parameters:\n",
    "        test_reviews (list): List of preprocessed test reviews.\n",
    "        word_indices (dict): Dictionary mapping words to their corresponding indices.\n",
    "\n",
    "    Returns:\n",
    "        bow_vectors (list): List of BoW representations for the input test reviews.\n",
    "    \"\"\"\n",
    "    max_sentence_length = 600\n",
    "    bow_vectors = []\n",
    "\n",
    "    for review in test_reviews:\n",
    "        if not isinstance(review, str):\n",
    "            tokens = ['#UNK#']  # Skip non-string elements\n",
    "        else:\n",
    "            tokens = review.split()\n",
    "        sent_indices = [word_indices.get(token, word_indices[\"#UNK#\"]) for token in tokens]\n",
    "        if len(sent_indices) < max_sentence_length:\n",
    "            sent_indices += [word_indices[\"#PAD#\"]] * (max_sentence_length - len(sent_indices))\n",
    "        bow_vectors.append(sent_indices)\n",
    "    return bow_vectors\n",
    "\n",
    "# Extract preprocessed test text reviews\n",
    "test_reviews = test_reviews_df['preprocessed_text']\n",
    "\n",
    "# Generate BoW representations for test reviews using pre-loaded word indices\n",
    "test_bow_vectors = BoW_test(test_reviews, word_indices)\n",
    "\n",
    "# Add BoW representations to the test DataFrame\n",
    "test_reviews_df['bow_vector'] = test_bow_vectors\n",
    "\n",
    "# Save the test DataFrame with BoW representations\n",
    "test_reviews_df.to_csv(Xtest_bow_textrep_reviews_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW GloVe Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_embeddings(glove_path):\n",
    "    \"\"\"\n",
    "    Load pre-trained GloVe word embeddings from a file and create an embedding dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        glove_path (str): Path to the GloVe embeddings file.\n",
    "\n",
    "    Returns:\n",
    "        emb_dict_glove (dict): Dictionary mapping words to their corresponding word vectors.\n",
    "        embeddings_glove (list): List of word vectors.\n",
    "    \"\"\"\n",
    "    embeddings_glove = []\n",
    "    emb_dict_glove = {}\n",
    "\n",
    "    # Open and read the GloVe embeddings file\n",
    "    with open(glove_path, 'rt', encoding='utf-8') as glove_file:\n",
    "        pretrained_vectors = glove_file.read().strip().split('\\n')\n",
    "\n",
    "    # Parse each line of the GloVe file to extract word vectors\n",
    "    for vector in range(len(pretrained_vectors)):\n",
    "        word = pretrained_vectors[vector].split(\"\\t\")[0]\n",
    "        word_vector = [float(val) for val in pretrained_vectors[vector].split('\\t')[1].split(' ')[0:]]\n",
    "        embeddings_glove.append(word_vector)\n",
    "        emb_dict_glove[word] = word_vector\n",
    "\n",
    "    return emb_dict_glove, embeddings_glove\n",
    "\n",
    "def get_glove_representation(reviews):\n",
    "    \"\"\"\n",
    "    Generate GloVe representations for a list of preprocessed text reviews.\n",
    "\n",
    "    Parameters:\n",
    "        reviews (list): List of preprocessed text reviews.\n",
    "\n",
    "    Returns:\n",
    "        glove_rep (list): List of GloVe representations for the input reviews.\n",
    "        word_indices (dict): Dictionary mapping words to their corresponding indices.\n",
    "    \"\"\"\n",
    "    max_sentence_length = 600\n",
    "\n",
    "    # Load pre-trained GloVe embeddings and create the embedding dictionary\n",
    "    emb_dict_glove_bow, embeddings = get_pretrained_embeddings(pretrained_glove_path)\n",
    "\n",
    "    # Add a zero vector for the '#PAD#' token\n",
    "    zeroes = [0 for i in range(max_sentence_length)]\n",
    "    embeddings.append(zeroes)\n",
    "\n",
    "    # Create word indices\n",
    "    word_indices = {}\n",
    "    idx_counter = 0\n",
    "    for word in emb_dict_glove_bow.keys():\n",
    "        word_indices[word] = idx_counter\n",
    "        idx_counter = idx_counter + 1\n",
    "    word_indices[\"#PAD#\"] = idx_counter\n",
    "    word_indices[\"#UNK#\"] = idx_counter + 1\n",
    "\n",
    "    # Generate GloVe representations for the input reviews\n",
    "    glove_rep = []\n",
    "    for review in reviews:\n",
    "        if not isinstance(review, str):\n",
    "            tokens.append('#UNK#')  # Skip non-string elements\n",
    "        else:\n",
    "            tokens = review.split()\n",
    "        sent_indices = [word_indices.get(token, word_indices[\"#UNK#\"]) for token in tokens]\n",
    "        if len(sent_indices) < max_sentence_length:\n",
    "            sent_indices += [word_indices[\"#PAD#\"]] * (max_sentence_length - len(sent_indices))\n",
    "        glove_rep.append(sent_indices)\n",
    "\n",
    "    return glove_rep, word_indices\n",
    "\n",
    "# Extract preprocessed text reviews\n",
    "reviews = train_reviews_df['preprocessed_text']\n",
    "\n",
    "# Generate GloVe representations and word indices\n",
    "glove_vectors, word_indices = get_glove_representation(reviews)\n",
    "\n",
    "# Add GloVe representations to the DataFrame\n",
    "train_reviews_df['glove_vector'] = glove_vectors\n",
    "\n",
    "# Save the word_indices to a file using pickle\n",
    "with open(glove_vocabulary_path, \"wb\") as f:\n",
    "    pickle.dump(word_indices, f)\n",
    "\n",
    "# Save the DataFrame with GloVe representations\n",
    "train_reviews_df.to_csv(Xtrain_glove_textrep_reviews_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already loaded the test reviews into 'test_reviews_df'\n",
    "# Load the app review data\n",
    "test_reviews_df = pd.read_csv(Xtest_data_path, encoding='latin-1')\n",
    "\n",
    "# Load the word_indices from the file using pickle\n",
    "with open(glove_vocabulary_path, \"rb\") as f:\n",
    "    word_indices = pickle.load(f)\n",
    "\n",
    "def get_glove_representation_test(reviews, word_indices):\n",
    "    \"\"\"\n",
    "    Generate GloVe representations for a list of preprocessed test reviews using pre-loaded word indices.\n",
    "\n",
    "    Parameters:\n",
    "        reviews (list): List of preprocessed test reviews.\n",
    "        word_indices (dict): Dictionary mapping words to their corresponding indices.\n",
    "\n",
    "    Returns:\n",
    "        glove_rep (list): List of GloVe representations for the input test reviews.\n",
    "    \"\"\"\n",
    "    max_sentence_length = 600\n",
    "\n",
    "    glove_rep = []\n",
    "    for review in reviews:\n",
    "        if not isinstance(review, str):\n",
    "            tokens.append('#UNK#')  # Skip non-string elements\n",
    "        else:\n",
    "            tokens = review.split()\n",
    "        sent_indices = [word_indices.get(token, word_indices[\"#UNK#\"]) for token in tokens]\n",
    "        if len(sent_indices) < max_sentence_length:\n",
    "            sent_indices += [word_indices[\"#PAD#\"]] * (max_sentence_length - len(sent_indices))\n",
    "        glove_rep.append(sent_indices)\n",
    "\n",
    "    return glove_rep\n",
    "\n",
    "# Extract preprocessed test text reviews\n",
    "test_reviews = test_reviews_df['preprocessed_text']\n",
    "\n",
    "# Generate GloVe representations for test reviews using pre-loaded word indices\n",
    "test_glove_vectors = get_glove_representation_test(test_reviews, word_indices)\n",
    "\n",
    "# Add GloVe representations to the test DataFrame\n",
    "test_reviews_df['glove_vector'] = test_glove_vectors\n",
    "\n",
    "# Save the test DataFrame with GloVe representations\n",
    "test_reviews_df.to_csv(Xtest_glove_textrep_reviews_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 5: Training and Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the string representation of bow_vector to a numerical array\n",
    "def parse_bow_vector(bow_str):\n",
    "    \"\"\"\n",
    "    Convert a string representation of a Bag of Words (BoW) vector to a numerical array.\n",
    "\n",
    "    Parameters:\n",
    "        bow_str (str): String representation of a BoW vector.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Numerical array representing the BoW vector.\n",
    "    \"\"\"\n",
    "    return np.array(ast.literal_eval(bow_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the string representation of glove_vector to a numerical array\n",
    "def parse_glove_vector(glove_str):\n",
    "    \"\"\"\n",
    "    Convert a string representation of a GloVe vector to a numerical array.\n",
    "\n",
    "    Parameters:\n",
    "        glove_str (str): String representation of a GloVe vector.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Numerical array representing the GloVe vector.\n",
    "    \"\"\"\n",
    "    return np.array(ast.literal_eval(glove_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine: BoW Random Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed training and test review data\n",
    "train_reviews_df = pd.read_csv(Xtrain_bow_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_bow_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Convert 'bow_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "\n",
    "# Perform classification with Support Vector Machine (SVM) using a linear kernel\n",
    "classifier_linear = svm.LinearSVC(C=0.1)\n",
    "\n",
    "# Perform cross-validation (5-fold cross-validation in this example)\n",
    "num_folds = 5\n",
    "t0 = time.time()\n",
    "scores = cross_val_score(classifier_linear, train_vectors, y_train, cv=num_folds, scoring='accuracy')\n",
    "t1 = time.time()\n",
    "time_cross_val = t1 - t0\n",
    "\n",
    "# Calculate the mean accuracy and standard deviation across all folds\n",
    "mean_accuracy = np.mean(scores)\n",
    "std_accuracy = np.std(scores)\n",
    "\n",
    "# Make predictions on the test data\n",
    "t0 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t1 = time.time()\n",
    "time_linear_predict = t1 - t0\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, prediction_linear)\n",
    "test_precision = precision_score(y_test, prediction_linear)\n",
    "test_recall = recall_score(y_test, prediction_linear)\n",
    "test_f1 = f1_score(y_test, prediction_linear)\n",
    "\n",
    "# Calculate train metrics\n",
    "train_prediction = classifier_linear.predict(train_vectors)\n",
    "train_accuracy = accuracy_score(y_train, train_prediction)\n",
    "train_precision = precision_score(y_train, train_prediction)\n",
    "train_recall = recall_score(y_train, train_prediction)\n",
    "train_f1 = f1_score(y_train, train_prediction)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)\n",
    "print(\"Time taken for training (Cross-validation):\", time_cross_val, \"seconds\")\n",
    "\n",
    "print(\"\\nTrain Metrics:\")\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Train Precision:\", train_precision)\n",
    "print(\"Train Recall:\", train_recall)\n",
    "print(\"Train F1-Score:\", train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine: BoW GloVe Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed training and test review data\n",
    "train_reviews_df = pd.read_csv(Xtrain_glove_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_glove_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Convert 'glove_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "\n",
    "# Perform classification with Support Vector Machine (SVM) using a linear kernel\n",
    "classifier_linear = svm.LinearSVC(C=0.1)\n",
    "\n",
    "# Perform cross-validation (5-fold cross-validation in this example)\n",
    "num_folds = 5\n",
    "t0 = time.time()\n",
    "scores = cross_val_score(classifier_linear, train_vectors, y_train, cv=num_folds, scoring='accuracy')\n",
    "t1 = time.time()\n",
    "time_cross_val = t1 - t0\n",
    "\n",
    "# Calculate the mean accuracy and standard deviation across all folds\n",
    "mean_accuracy = np.mean(scores)\n",
    "std_accuracy = np.std(scores)\n",
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "t0 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t1 = time.time()\n",
    "time_linear_predict = t1 - t0\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, prediction_linear)\n",
    "test_precision = precision_score(y_test, prediction_linear)\n",
    "test_recall = recall_score(y_test, prediction_linear)\n",
    "test_f1 = f1_score(y_test, prediction_linear)\n",
    "\n",
    "# Calculate train metrics\n",
    "train_prediction = classifier_linear.predict(train_vectors)\n",
    "train_accuracy = accuracy_score(y_train, train_prediction)\n",
    "train_precision = precision_score(y_train, train_prediction)\n",
    "train_recall = recall_score(y_train, train_prediction)\n",
    "train_f1 = f1_score(y_train, train_prediction)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)\n",
    "print(\"Time taken for training (Cross-validation):\", time_cross_val, \"seconds\")\n",
    "\n",
    "print(\"\\nTrain Metrics:\")\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Train Precision:\", train_precision)\n",
    "print(\"Train Recall:\", train_recall)\n",
    "print(\"Train F1-Score:\", train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes: BoW Random Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed training and test review data\n",
    "train_reviews_df = pd.read_csv(Xtrain_bow_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_bow_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Convert 'bow_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Perform cross-validation (5-fold cross-validation in this example)\n",
    "num_folds = 5\n",
    "t0 = time.time()\n",
    "scores = cross_val_score(classifier, train_vectors, y_train, cv=num_folds, scoring='accuracy')\n",
    "t1 = time.time()\n",
    "time_cross_val = t1 - t0\n",
    "\n",
    "# Calculate the mean accuracy and standard deviation across all folds\n",
    "mean_accuracy = np.mean(scores)\n",
    "std_accuracy = np.std(scores)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = classifier.predict(test_vectors)\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "test_precision = precision_score(y_test, predictions, zero_division=1)  # To handle UndefinedMetricWarning\n",
    "test_recall = recall_score(y_test, predictions, zero_division=1)\n",
    "test_f1 = f1_score(y_test, predictions, zero_division=1)\n",
    "\n",
    "# Make predictions on the train data\n",
    "train_predictions = classifier.predict(train_vectors)\n",
    "\n",
    "# Calculate train metrics\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision = precision_score(y_train, train_predictions, zero_division=1)\n",
    "train_recall = recall_score(y_train, train_predictions, zero_division=1)\n",
    "train_f1 = f1_score(y_train, train_predictions, zero_division=1)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)\n",
    "print(\"Time taken for training (Cross-validation):\", time_cross_val, \"seconds\")\n",
    "\n",
    "print(\"\\nTrain Metrics:\")\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Train Precision:\", train_precision)\n",
    "print(\"Train Recall:\", train_recall)\n",
    "print(\"Train F1-Score:\", train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes: BoW GloVe Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed training and test review data with GloVe representations\n",
    "train_reviews_df = pd.read_csv(Xtrain_glove_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_glove_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Convert 'glove_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Perform cross-validation (5-fold cross-validation in this example)\n",
    "num_folds = 5\n",
    "t0 = time.time()\n",
    "scores = cross_val_score(classifier, train_vectors, y_train, cv=num_folds, scoring='accuracy')\n",
    "t1 = time.time()\n",
    "time_cross_val = t1 - t0\n",
    "\n",
    "# Calculate the mean accuracy and standard deviation across all folds\n",
    "mean_accuracy = np.mean(scores)\n",
    "std_accuracy = np.std(scores)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = classifier.predict(test_vectors)\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "test_precision = precision_score(y_test, predictions, zero_division=1)  # To handle UndefinedMetricWarning\n",
    "test_recall = recall_score(y_test, predictions, zero_division=1)\n",
    "test_f1 = f1_score(y_test, predictions, zero_division=1)\n",
    "\n",
    "# Make predictions on the train data\n",
    "train_predictions = classifier.predict(train_vectors)\n",
    "\n",
    "# Calculate train metrics\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision = precision_score(y_train, train_predictions, zero_division=1)\n",
    "train_recall = recall_score(y_train, train_predictions, zero_division=1)\n",
    "train_f1 = f1_score(y_train, train_predictions, zero_division=1)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)\n",
    "print(\"Time taken for training (Cross-validation):\", time_cross_val, \"seconds\")\n",
    "\n",
    "print(\"\\nTrain Metrics:\")\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Train Precision:\", train_precision)\n",
    "print(\"Train Recall:\", train_recall)\n",
    "print(\"Train F1-Score:\", train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Entropy: BoW Random Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed training and test review data with Bag of Words (BoW) representations\n",
    "train_reviews_df = pd.read_csv(Xtrain_bow_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_bow_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Function to convert the string representation of bow_vector to a numerical array\n",
    "def parse_bow_vector(bow_str):\n",
    "    \"\"\"\n",
    "    Convert a string representation of a BoW vector to a numerical array.\n",
    "\n",
    "    Parameters:\n",
    "        bow_str (str): String representation of a BoW vector.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Numerical array representing the BoW vector.\n",
    "    \"\"\"\n",
    "    return np.array(ast.literal_eval(bow_str))\n",
    "\n",
    "# Convert 'bow_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "\n",
    "# Initialize the Maximum Entropy (Logistic Regression) model\n",
    "# Setting multi_class='multinomial' makes it behave like a MaxEnt model\n",
    "maxent_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
    "\n",
    "# Perform cross-validation (5-fold cross-validation in this example)\n",
    "num_folds = 5\n",
    "t0 = time.time()\n",
    "scores = cross_val_score(maxent_model, train_vectors, y_train, cv=num_folds, scoring='accuracy')\n",
    "t1 = time.time()\n",
    "time_cross_val = t1 - t0\n",
    "\n",
    "# Calculate the mean accuracy and standard deviation across all folds\n",
    "mean_accuracy = np.mean(scores)\n",
    "std_accuracy = np.std(scores)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = maxent_model.predict(test_vectors)\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "test_precision = precision_score(y_test, predictions)\n",
    "test_recall = recall_score(y_test, predictions)\n",
    "test_f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# Make predictions on the train data\n",
    "train_predictions = maxent_model.predict(train_vectors)\n",
    "\n",
    "# Calculate train metrics\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision = precision_score(y_train, train_predictions)\n",
    "train_recall = recall_score(y_train, train_predictions)\n",
    "train_f1 = f1_score(y_train, train_predictions)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)\n",
    "print(\"Time taken for training (Cross-validation):\", time_cross_val, \"seconds\")\n",
    "\n",
    "print(\"\\nTrain Metrics:\")\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Train Precision:\", train_precision)\n",
    "print(\"Train Recall:\", train_recall)\n",
    "print(\"Train F1-Score:\", train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Entropy: BoW GloVe Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed training and test review data with GloVe vector representations\n",
    "train_reviews_df = pd.read_csv(Xtrain_glove_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_glove_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Convert 'glove_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "\n",
    "# Initialize the Maximum Entropy (Logistic Regression) model\n",
    "# Setting multi_class='multinomial' makes it behave like a MaxEnt model\n",
    "maxent_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
    "\n",
    "# Perform cross-validation (5-fold cross-validation in this example)\n",
    "num_folds = 5\n",
    "t0 = time.time()\n",
    "scores = cross_val_score(maxent_model, train_vectors, y_train, cv=num_folds, scoring='accuracy')\n",
    "t1 = time.time()\n",
    "time_cross_val = t1 - t0\n",
    "\n",
    "# Calculate the mean accuracy and standard deviation across all folds\n",
    "mean_accuracy = np.mean(scores)\n",
    "std_accuracy = np.std(scores)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = maxent_model.predict(test_vectors)\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "test_precision = precision_score(y_test, predictions)\n",
    "test_recall = recall_score(y_test, predictions)\n",
    "test_f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# Make predictions on the train data\n",
    "train_predictions = maxent_model.predict(train_vectors)\n",
    "\n",
    "# Calculate train metrics\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision = precision_score(y_train, train_predictions)\n",
    "train_recall = recall_score(y_train, train_predictions)\n",
    "train_f1 = f1_score(y_train, train_predictions)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)\n",
    "print(\"Time taken for training (Cross-validation):\", time_cross_val, \"seconds\")\n",
    "\n",
    "print(\"\\nTrain Metrics:\")\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Train Precision:\", train_precision)\n",
    "print(\"Train Recall:\", train_recall)\n",
    "print(\"Train F1-Score:\", train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree: BoW Random Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset paths (Xtrain_bow_textrep_reviews_path, Xtest_bow_textrep_reviews_path) and y_train here\n",
    "\n",
    "# Load the training and test data\n",
    "train_reviews_df = pd.read_csv(Xtrain_bow_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_bow_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Convert 'bow_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "\n",
    "# y_train = train_reviews_df['label']\n",
    "# y_test = test_reviews_df['label']\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Perform k-fold cross-validation and calculate metrics\n",
    "kfold = 5  # Adjust the number of folds as needed\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score using cross_val_score\n",
    "scores = cross_val_score(classifier, train_vectors, y_train, cv=kfold, scoring='accuracy')\n",
    "accuracy = scores.mean()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "precision_scores = cross_val_score(classifier, train_vectors, y_train, cv=kfold, scoring='precision_macro')\n",
    "precision = precision_scores.mean()\n",
    "\n",
    "recall_scores = cross_val_score(classifier, train_vectors, y_train, cv=kfold, scoring='recall_macro')\n",
    "recall = recall_scores.mean()\n",
    "\n",
    "f1_scores = cross_val_score(classifier, train_vectors, y_train, cv=kfold, scoring='f1_macro')\n",
    "f1 = f1_scores.mean()\n",
    "\n",
    "# Print the metrics\n",
    "print(\"K-Fold Cross-Validation Results:\")\n",
    "print(\"Mean Accuracy:\", accuracy)\n",
    "print(\"Mean Precision:\", precision)\n",
    "print(\"Mean Recall:\", recall)\n",
    "print(\"Mean F1-Score:\", f1)\n",
    "print(\"Time taken for training and testing:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = classifier.predict(test_vectors)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "test_precision = precision_score(y_test, predictions, average='macro')\n",
    "test_recall = recall_score(y_test, predictions, average='macro')\n",
    "test_f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "# Print the test metrics\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree: BoW GloVe Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier for Text Classification\n",
    "\n",
    "# Load the training and test data\n",
    "train_reviews_df = pd.read_csv(Xtrain_glove_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_glove_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Convert 'glove_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Perform k-fold cross-validation and calculate metrics\n",
    "kfold = 5  # Adjust the number of folds as needed\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score using cross_val_score\n",
    "scores = cross_val_score(classifier, train_vectors, y_train, cv=kfold, scoring='accuracy')\n",
    "accuracy = scores.mean()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "precision_scores = cross_val_score(classifier, train_vectors, y_train, cv=kfold, scoring='precision_macro')\n",
    "precision = precision_scores.mean()\n",
    "\n",
    "recall_scores = cross_val_score(classifier, train_vectors, y_train, cv=kfold, scoring='recall_macro')\n",
    "recall = recall_scores.mean()\n",
    "\n",
    "f1_scores = cross_val_score(classifier, train_vectors, y_train, cv=kfold, scoring='f1_macro')\n",
    "f1 = f1_scores.mean()\n",
    "\n",
    "# Print the metrics\n",
    "print(\"K-Fold Cross-Validation Results:\")\n",
    "print(\"Mean Accuracy:\", accuracy)\n",
    "print(\"Mean Precision:\", precision)\n",
    "print(\"Mean Recall:\", recall)\n",
    "print(\"Mean F1-Score:\", f1)\n",
    "print(\"Time taken for training and testing:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = classifier.predict(test_vectors)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "test_precision = precision_score(y_test, predictions, average='macro')\n",
    "test_recall = recall_score(y_test, predictions, average='macro')\n",
    "test_f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "# Print the test metrics\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network: BoW Random Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for Text Classification using Bag-of-Words (BoW) Representation\n",
    "\n",
    "# Load the training and test data\n",
    "train_reviews_df = pd.read_csv(Xtrain_bow_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_bow_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "num_classes = 2  # Number of classes (binary classification)\n",
    "\n",
    "# Convert 'bow_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['bow_vector'].apply(parse_bow_vector).values)\n",
    "\n",
    "# Initialize the Neural Network model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(500, input_dim=train_vectors.shape[1], activation='sigmoid'))  # Input layer\n",
    "model.add(Dense(num_classes, activation='sigmoid'))  # Output layer with 'num_classes' neurons\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# One-hot encode the target labels for binary classification\n",
    "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Train the model with the training data\n",
    "start_time = time.time()\n",
    "model.fit(train_vectors, y_train_onehot, epochs=5, batch_size=8, validation_split=0.1)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_onehot = model.predict(test_vectors)\n",
    "# Convert the one-hot encoded predictions back to class labels\n",
    "predictions = np.argmax(predictions_onehot, axis=1)\n",
    "\n",
    "# Calculate metrics on the test data\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "test_precision = precision_score(y_test, predictions, average='macro')\n",
    "test_recall = recall_score(y_test, predictions, average='macro')\n",
    "test_f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "# Print the test metrics\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)\n",
    "print(\"Time taken for training:\", training_time, \"seconds\")\n",
    "\n",
    "# Calculate metrics on the train data\n",
    "train_predictions_onehot = model.predict(train_vectors)\n",
    "train_predictions = np.argmax(train_predictions_onehot, axis=1)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision = precision_score(y_train, train_predictions, average='macro')\n",
    "train_recall = recall_score(y_train, train_predictions, average='macro')\n",
    "train_f1 = f1_score(y_train, train_predictions, average='macro')\n",
    "\n",
    "# Print the train metrics\n",
    "print(\"Train Metrics:\")\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Train Precision:\", train_precision)\n",
    "print(\"Train Recall:\", train_recall)\n",
    "print(\"Train F1-Score:\", train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network: BoW GloVe Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for Text Classification using GloVe Word Embeddings\n",
    "\n",
    "num_classes = 2  # Number of classes (binary classification)\n",
    "\n",
    "# Load the training and test data\n",
    "train_reviews_df = pd.read_csv(Xtrain_glove_textrep_reviews_path, encoding='latin-1')\n",
    "test_reviews_df = pd.read_csv(Xtest_glove_textrep_reviews_path, encoding='latin-1')\n",
    "\n",
    "# Convert 'glove_vector' column to numerical arrays\n",
    "train_vectors = np.vstack(train_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "test_vectors = np.vstack(test_reviews_df['glove_vector'].apply(parse_glove_vector).values)\n",
    "\n",
    "# Initialize the Neural Network model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(128, input_dim=train_vectors.shape[1], activation='relu'))  # Input layer\n",
    "model.add(Dense(64, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Output layer with 'num_classes' neurons\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# One-hot encode the target labels for multi-class classification\n",
    "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Train the model with the training data\n",
    "start_time = time.time()\n",
    "model.fit(train_vectors, y_train_onehot, epochs=10, batch_size=4, validation_split=0.1)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_onehot = model.predict(test_vectors)\n",
    "# Convert the one-hot encoded predictions back to class labels\n",
    "predictions = np.argmax(predictions_onehot, axis=1)\n",
    "\n",
    "# Calculate metrics on the test data\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "test_precision = precision_score(y_test, predictions, average='macro')\n",
    "test_recall = recall_score(y_test, predictions, average='macro')\n",
    "test_f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "# Print the test metrics\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-Score:\", test_f1)\n",
    "print(\"Time taken for training:\", training_time, \"seconds\")\n",
    "\n",
    "# Calculate metrics on the train data\n",
    "train_predictions_onehot = model.predict(train_vectors)\n",
    "train_predictions = np.argmax(train_predictions_onehot, axis=1)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision = precision_score(y_train, train_predictions, average='macro')\n",
    "train_recall = recall_score(y_train, train_predictions, average='macro')\n",
    "train_f1 = f1_score(y_train, train_predictions, average='macro')\n",
    "\n",
    "# Print the train metrics\n",
    "print(\"Train Metrics:\")\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Train Precision:\", train_precision)\n",
    "print(\"Train Recall:\", train_recall)\n",
    "print(\"Train F1-Score:\", train_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
